
# Project 

The project involved parsing the website https://www.copart.com/ to gather information about job vacancies. The libraries used for this task were requests, BeautifulSoup, and Selenium.

# Description


Using Selenium and Python, this parser collects data on a multitude of vehicles presented on the copart.com website. The script automatically navigates to each page and extracts information about the make, model, year of manufacture, VIN code, and damage status of each vehicle. This allows for quick and efficient collection of data on a large number of vehicles for subsequent analysis.




# Technologies Used

-   Python

> The main programming language used for the project.

-   requests

> Python library used for making HTTP requests to the server.

-   BeautifulSoup

> Python library used for parsing HTML and XML documents.

-   Selenium

> Automation tool used for interacting with web browsers.

-   Google Chrome

> Web browser used for the project, since it is natively supported by Selenium.

-   Git

> Version control system used for tracking changes in the code.

-   GitHub

> Web-based hosting service used for version control and collaborative software development.

# How to Run

-   Clone the repository
-   Create a virtual environment and activate it
-   Install the required packages using the command `pip install -r requirements.txt`
-   Run the bot using the command `python main.py`

# Contributing

If you want to contribute to the project, you can fork the repository, make your changes, and submit a pull request. Please make sure to follow the coding style used in the project and include tests for your changes.
